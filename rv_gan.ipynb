{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWzAJkDcwheiKk+LXhf8rD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/e-mny/drive_retinal_seg/blob/main/rv_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RV-GAN: Segmenting Retinal Vascular Structure in Fundus Photographs using a Novel Multi-scale Generative Adversarial Network\n",
        "\n",
        "Original paper was written in TensorFlow, so I will be practise using PyTorch.\n",
        "\n",
        "I will also use this chance to try PyTorch Lightning to speed up the workflow."
      ],
      "metadata": {
        "id": "C9TcJhluqdFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "from lightning.pytorch.callbacks import ModelSummary\n",
        "from torch.utils.data import DataLoader, random_split, Dataset\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "from sklearn.model_selection import KFold\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2"
      ],
      "metadata": {
        "id": "U4Rrend8s0nR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95109dcd-0c06-408c-b418-2c81bad6cb68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightning in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]<2025.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2023.6.0)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (0.10.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.23.5)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (23.2)\n",
            "Requirement already satisfied: torch<4.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.3.0.post0)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.5.0)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning) (2.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (3.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.8.0->lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (2.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=1.12.0->lightning) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=1.12.0->lightning) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Dataset Name | Total Available Images | Training Images Used | Testing Images Used\n",
        "| -------- | -------- | -------- | ------- |\n",
        "| DRIVE  | 40 | 20 | 20\n",
        "| CHASE  | 28 | 20 | 8\n",
        "| STARE  | 400  | 20 | 4\n"
      ],
      "metadata": {
        "id": "8VO1uziA8y4T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STARE Dataset"
      ],
      "metadata": {
        "id": "_VTdwypuqNm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting STARE Dataset\n",
        "\n",
        "!rm -rf STARE\n",
        "!mkdir STARE\n",
        "!mkdir STARE/images\n",
        "!mkdir STARE/labels\n",
        "!wget https://cecas.clemson.edu/~ahoover/stare/probing/stare-images.tar\n",
        "!wget https://cecas.clemson.edu/~ahoover/stare/probing/labels-ah.tar\n",
        "!tar xf stare-images.tar -C STARE/images\n",
        "!tar xf labels-ah.tar -C STARE/labels\n",
        "\n",
        "# Unzip the ppm.gz zipped files in STARE folder\n",
        "!find STARE/images/ -type f -name \"*.gz\" -exec gzip -d {} +\n",
        "!find STARE/labels/ -type f -name \"*.gz\" -exec gzip -d {} +"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hMKVB9y_UO7",
        "outputId": "7f2a0d35-0a83-47e9-9164-da58a1a62912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘STARE/train/images’: No such file or directory\n",
            "mkdir: cannot create directory ‘STARE/train/labels’: No such file or directory\n",
            "mkdir: cannot create directory ‘STARE/test/images’: No such file or directory\n",
            "mkdir: cannot create directory ‘STARE/test/labels’: No such file or directory\n",
            "--2024-01-31 09:36:55--  https://cecas.clemson.edu/~ahoover/stare/probing/stare-images.tar\n",
            "Resolving cecas.clemson.edu (cecas.clemson.edu)... 130.127.200.74\n",
            "Connecting to cecas.clemson.edu (cecas.clemson.edu)|130.127.200.74|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18674176 (18M) [application/x-tar]\n",
            "Saving to: ‘stare-images.tar’\n",
            "\n",
            "stare-images.tar    100%[===================>]  17.81M  48.8MB/s    in 0.4s    \n",
            "\n",
            "2024-01-31 09:36:55 (48.8 MB/s) - ‘stare-images.tar’ saved [18674176/18674176]\n",
            "\n",
            "--2024-01-31 09:36:55--  https://cecas.clemson.edu/~ahoover/stare/probing/labels-ah.tar\n",
            "Resolving cecas.clemson.edu (cecas.clemson.edu)... 130.127.200.74\n",
            "Connecting to cecas.clemson.edu (cecas.clemson.edu)|130.127.200.74|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 241664 (236K) [application/x-tar]\n",
            "Saving to: ‘labels-ah.tar’\n",
            "\n",
            "labels-ah.tar       100%[===================>] 236.00K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2024-01-31 09:36:55 (2.68 MB/s) - ‘labels-ah.tar’ saved [241664/241664]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_train_test_stare(source_folder, train_folder, test_folder, num_test_images):\n",
        "    # Create images and labels folder in train and test folders if they don't exist\n",
        "    os.makedirs(os.path.join(train_folder, \"images\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(test_folder, \"images\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(train_folder, \"labels\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(test_folder, \"labels\"), exist_ok=True)\n",
        "    image_folder = os.path.join(source_folder, \"images\")\n",
        "    label_folder = os.path.join(source_folder, \"labels\")\n",
        "\n",
        "    # Get a list of all image files in the image folder\n",
        "    image_files = [f for f in os.listdir(image_folder) if f.lower().endswith('.ppm')]\n",
        "\n",
        "    # Get a list of all label files in the label folder\n",
        "    label_files = [f for f in os.listdir(label_folder) if f.lower().endswith('.ppm')]\n",
        "\n",
        "    # Randomly choose images for the test set\n",
        "    test_images = random.sample(image_files, num_test_images)\n",
        "    test_file_name_list = []\n",
        "\n",
        "    # Move images to the appropriate folders\n",
        "    for image_file in image_files:\n",
        "        source_path = os.path.join(image_folder, image_file)\n",
        "        if image_file in test_images:\n",
        "            test_file_name_list.append(os.path.splitext(image_file)[0])\n",
        "            destination_path = os.path.join(test_folder, \"images\", image_file)\n",
        "        else:\n",
        "            destination_path = os.path.join(train_folder, \"images\", image_file)\n",
        "        shutil.move(source_path, destination_path)\n",
        "\n",
        "    for label_file in label_files:\n",
        "        source_path = os.path.join(label_folder, label_file)\n",
        "        if label_file.split(\".\")[0] in test_file_name_list:\n",
        "            destination_path = os.path.join(test_folder, \"labels\", label_file)\n",
        "        else:\n",
        "            destination_path = os.path.join(train_folder, \"labels\", label_file)\n",
        "        shutil.move(source_path, destination_path)\n",
        "\n",
        "\n",
        "    os.rmdir(os.path.join(source_folder, \"images\"))\n",
        "    os.rmdir(os.path.join(source_folder, \"labels\"))\n",
        "\n",
        "# STARE (4 images used for testing)\n",
        "stare_source_folder = '/content/STARE'\n",
        "stare_train_folder = '/content/STARE/training'\n",
        "stare_test_folder = '/content/STARE/test'\n",
        "split_train_test_stare(stare_source_folder, stare_train_folder, stare_test_folder, num_test_images = 4)"
      ],
      "metadata": {
        "id": "-Vj8vLDS_6-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CHASE Dataset"
      ],
      "metadata": {
        "id": "7o0x4N5QqQ9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting CHASE Dataset\n",
        "\n",
        "!rm -rf CHASE\n",
        "!mkdir CHASE\n",
        "!wget https://staffnet.kingston.ac.uk/~ku15565/CHASE_DB1/assets/CHASEDB1.zip\n",
        "!unzip CHASEDB1.zip -d CHASE"
      ],
      "metadata": {
        "id": "TLZoHxvaq8yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_image_labels(source_folder):\n",
        "  image_folder = os.path.join(source_folder, \"images\")\n",
        "  labels_folder = os.path.join(source_folder, \"labels\")\n",
        "  os.makedirs(image_folder, exist_ok=True)\n",
        "  os.makedirs(labels_folder, exist_ok=True)\n",
        "  for filename in os.listdir(source_folder):\n",
        "\n",
        "    # Images end with .jpg file\n",
        "    if filename.lower().endswith(\".jpg\"):\n",
        "      curr_file_path = os.path.join(source_folder, filename)\n",
        "      new_file_path = os.path.join(image_folder, filename)\n",
        "      shutil.move(curr_file_path, new_file_path)\n",
        "    # Labels end with .png file\n",
        "    elif filename.lower().endswith(\".png\"):\n",
        "      curr_file_path = os.path.join(source_folder, filename)\n",
        "      new_file_path = os.path.join(labels_folder, filename)\n",
        "      shutil.move(curr_file_path, new_file_path)\n",
        "\n",
        "  print(\"Files split successfully.\")\n",
        "\n",
        "\n",
        "def rename_files(source_folder):\n",
        "  # Create a mapping of old names to new unique IDs\n",
        "  id_mapping = {}\n",
        "  unique_id = 1\n",
        "\n",
        "  for file_name in sorted(os.listdir(source_folder)):\n",
        "      # Extract the patient number from the folder name\n",
        "      patient_number = file_name[6:9]\n",
        "\n",
        "      # Create a unique ID if not already assigned\n",
        "      if patient_number not in id_mapping:\n",
        "          id_mapping[patient_number] = unique_id\n",
        "          unique_id += 1\n",
        "\n",
        "      # File extension\n",
        "      # old_file_name, file_extension = os.path.splitext(file_name)\n",
        "\n",
        "      # Construct the new folder name\n",
        "      new_file_name = f\"{id_mapping[patient_number]:02d}{file_name[9:]}\"\n",
        "\n",
        "      # Rename the folder\n",
        "      old_path = os.path.join(source_folder, file_name)\n",
        "      new_path = os.path.join(source_folder, new_file_name)\n",
        "      os.rename(old_path, new_path)\n",
        "\n",
        "  print(\"Folders renamed successfully.\")\n",
        "\n",
        "\n",
        "def split_train_test_chase(source_folder, train_folder, test_folder, num_test_images):\n",
        "    # Create images and labels folder in train and test folders if they don't exist\n",
        "    os.makedirs(os.path.join(train_folder, \"images\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(test_folder, \"images\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(train_folder, \"labels\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(test_folder, \"labels\"), exist_ok=True)\n",
        "\n",
        "    image_folder = os.path.join(source_folder, \"images\")\n",
        "    label_folder = os.path.join(source_folder, \"labels\")\n",
        "\n",
        "    # Get a list of all image files in the image folder\n",
        "    image_files = [f for f in os.listdir(image_folder) if f.lower().endswith('.jpg')]\n",
        "\n",
        "    # Get a list of all label files in the label folder\n",
        "    label_files = [f for f in os.listdir(label_folder) if f.lower().endswith('.png')]\n",
        "\n",
        "    # Randomly choose images for the test set\n",
        "    test_images = random.sample(image_files, num_test_images)\n",
        "\n",
        "    test_file_name_list = []\n",
        "\n",
        "    # Move images to the appropriate folders\n",
        "    for image_file in image_files:\n",
        "        source_path = os.path.join(image_folder, image_file)\n",
        "        if image_file in test_images:\n",
        "            test_file_name_list.append(os.path.splitext(image_file)[0])\n",
        "            destination_path = os.path.join(test_folder, \"images\", image_file)\n",
        "        else:\n",
        "            destination_path = os.path.join(train_folder, \"images\", image_file)\n",
        "        shutil.move(source_path, destination_path)\n",
        "\n",
        "    for label_file in label_files:\n",
        "        source_path = os.path.join(label_folder, label_file)\n",
        "        if label_file.split(\"_\")[0] in test_file_name_list:\n",
        "            destination_path = os.path.join(test_folder, \"labels\", label_file)\n",
        "        else:\n",
        "            destination_path = os.path.join(train_folder, \"labels\", label_file)\n",
        "        shutil.move(source_path, destination_path)\n",
        "\n",
        "\n",
        "    os.rmdir(os.path.join(source_folder, \"images\"))\n",
        "    os.rmdir(os.path.join(source_folder, \"labels\"))\n",
        "\n",
        "# CHASE (8 images used for testing)\n",
        "chase_source_folder = '/content/CHASE'\n",
        "chase_train_folder = '/content/CHASE/training'\n",
        "chase_test_folder = '/content/CHASE/test'\n",
        "rename_files(chase_source_folder)\n",
        "split_image_labels(chase_source_folder)\n",
        "split_train_test_chase(chase_source_folder, chase_train_folder, chase_test_folder, num_test_images = 8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbkSyJGK68-B",
        "outputId": "b1acab1a-9e1b-493e-b5f4-7857df016854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folders renamed successfully.\n",
            "Files split successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DRIVE Dataset\n",
        "\n",
        "Remember to upload your Kaggle.json file when prompted"
      ],
      "metadata": {
        "id": "DHkCKCYhqUMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading DRIVE dataset\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!rm -r ~/.kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!mv ./kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d andrewmvd/drive-digital-retinal-images-for-vessel-extraction\n",
        "!mkdir DRIVE_patches\n",
        "!unzip drive-digital-retinal-images-for-vessel-extraction.zip\n",
        "!mv DRIVE/training/1st_manual DRIVE/training/labels"
      ],
      "metadata": {
        "id": "5-HcwdQ4roOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_image_patches(folder_path, output_path, patch_size=128, stride=32):\n",
        "  for root, _, files in os.walk(folder_path):\n",
        "    for filename in files:\n",
        "      if filename.endswith((\".tif\", \".jpg\", \".ppm\")):\n",
        "        curr_file_path = os.path.join(root, filename)\n",
        "        # Open the image\n",
        "        img = Image.open(curr_file_path)\n",
        "\n",
        "        # Convert image to numpy array\n",
        "        img_array = np.array(img)\n",
        "\n",
        "        # Get image shape\n",
        "        height, width, channels = img_array.shape\n",
        "        # print(f\"Original Image Shape: {height, width}\")\n",
        "\n",
        "\n",
        "        # Create output directory if it doesn't exist\n",
        "        os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "        # Iterate over the image with the specified stride\n",
        "        for y in range(0, height - patch_size + 1, stride):\n",
        "            for x in range(0, width - patch_size + 1, stride):\n",
        "                # Extract the patch\n",
        "                patch = img_array[y:y+patch_size, x:x+patch_size, :]\n",
        "\n",
        "\n",
        "                # Save the patch as an image\n",
        "                patch_img = Image.fromarray(patch.astype('uint8'))\n",
        "                new_filename = filename.split(\".\")[0]\n",
        "                patch_filename = f\"{new_filename}{y}_{x}.png\"\n",
        "                patch_img.save(os.path.join(output_path, patch_filename))\n",
        "\n",
        "\n",
        "def generate_FOV_mask(source_dir):\n",
        "  image_folder = os.path.join(source_dir, \"images\")\n",
        "  mask_folder = os.path.join(source_dir, \"mask\")\n",
        "  os.makedirs(mask_folder, exist_ok=True)\n",
        "\n",
        "  for filename in os.listdir(image_folder):\n",
        "    image_path = os.path.join(image_folder, filename)\n",
        "    output_path = os.path.join(mask_folder, filename)\n",
        "\n",
        "    # Read the image\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Convert the image to grayscale\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply a simple threshold to identify pixels close to black\n",
        "    _, thresholded_image = cv2.threshold(gray_image, 30, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Save the thresholded image\n",
        "    cv2.imwrite(output_path, thresholded_image)\n",
        "  print(f\"Generated FOV mask for {source_dir}\")\n",
        "\n",
        "\n",
        "def rename_masks(source_dir):\n",
        "  for filename in os.listdir(source_dir):\n",
        "    # Check if the filename contains \"_mask\"\n",
        "    if \"_mask\" in filename:\n",
        "        # Create the new filename without \"_mask\"\n",
        "        new_filename = filename.replace(\"_mask\", \"\")\n",
        "\n",
        "        # Construct the full paths\n",
        "        old_path = os.path.join(source_dir, filename)\n",
        "        new_path = os.path.join(source_dir, new_filename)\n",
        "\n",
        "        # Rename the file\n",
        "        os.rename(old_path, new_path)\n"
      ],
      "metadata": {
        "id": "4h0PRQ3107sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Training Dataset\n",
        "\n",
        "I will prepare the image patches here and also generate FOV masks for CHASE and STARE as they were not included in the dataset.\n",
        "\n",
        "I will also rename the mask files in DRIVE to be the same as the original image\n",
        "\n",
        "```\n",
        "\"xx_training_mask.gif\" -> \"xx_training.gif\"\n",
        "```\n",
        "\n",
        "where xx is the index of the image"
      ],
      "metadata": {
        "id": "cRpZoonE3zax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data preparation\n",
        "PATCH_SIZE = 128\n",
        "TRAIN_STRIDE = 32\n",
        "\n",
        "# DRIVE\n",
        "drive_original_train_dir = '/content/DRIVE/training/images/'\n",
        "drive_patches_train_dir = '/content/DRIVE_patches/training'\n",
        "create_image_patches(drive_original_train_dir, drive_patches_train_dir, PATCH_SIZE, TRAIN_STRIDE)\n",
        "rename_masks('/content/DRIVE/training/mask/')\n",
        "\n",
        "\n",
        "print(f\"DRIVE dataset has {len(os.listdir(drive_patches_train_dir))} patches\")\n",
        "\n",
        "# CHASE\n",
        "chase_original_train_dir = '/content/CHASE/training/images/'\n",
        "chase_patches_train_dir = '/content/CHASE_patches/training'\n",
        "create_image_patches(chase_original_train_dir, chase_patches_train_dir, PATCH_SIZE, TRAIN_STRIDE)\n",
        "generate_FOV_mask(\"/content/CHASE/training/\")\n",
        "print(f\"CHASE dataset has {len(os.listdir(chase_patches_train_dir))} patches\")\n",
        "\n",
        "# STARE\n",
        "stare_original_train_dir = '/content/STARE/training/images/'\n",
        "stare_patches_train_dir = '/content/STARE_patches/training'\n",
        "create_image_patches(stare_original_train_dir, stare_patches_train_dir, PATCH_SIZE, TRAIN_STRIDE)\n",
        "generate_FOV_mask(\"/content/STARE/training/\")\n",
        "print(f\"STARE dataset has {len(os.listdir(stare_patches_train_dir))} patches\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOOAiscs25Fp",
        "outputId": "b5e0d875-058a-4a42-9398-acfcd7350108"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DRIVE dataset has 4200 patches\n",
            "CHASE dataset has 15120 patches\n",
            "STARE dataset has 4320 patches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Test Dataset\n",
        "\n",
        "Generating image patches for test dataset takes very long because of the short stride"
      ],
      "metadata": {
        "id": "POlSypvlqaje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_STRIDE = 3\n",
        "\n",
        "# DRIVE\n",
        "drive_original_test_dir = '/content/DRIVE/test/images/'\n",
        "drive_patches_test_dir = '/content/DRIVE_patches/test'\n",
        "create_image_patches(drive_original_test_dir, drive_patches_test_dir, PATCH_SIZE, TEST_STRIDE)\n",
        "\n",
        "print(f\"DRIVE dataset has {len(os.listdir(drive_patches_test_dir))} patches\")\n",
        "\n",
        "# CHASE\n",
        "chase_original_test_dir = '/content/CHASE/test/images/'\n",
        "chase_patches_test_dir = '/content/CHASE_patches/test'\n",
        "create_image_patches(chase_original_test_dir, chase_patches_test_dir, PATCH_SIZE, TEST_STRIDE)\n",
        "print(f\"CHASE dataset has {len(os.listdir(chase_patches_test_dir))} patches\")\n",
        "\n",
        "# STARE\n",
        "stare_original_test_dir = '/content/STARE/test/images/'\n",
        "stare_patches_test_dir = '/content/STARE_patches/test'\n",
        "create_image_patches(stare_original_test_dir, stare_patches_test_dir, PATCH_SIZE, TEST_STRIDE)\n",
        "print(f\"STARE dataset has {len(os.listdir(stare_patches_test_dir))} patches\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "O9ctvdUDq7gx",
        "outputId": "029c0142-b58a-4e64-e947-966cf0d1e5c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DRIVE dataset has 446760 patches\n",
            "CHASE dataset has 647184 patches\n",
            "STARE dataset has 122240 patches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WholeImageDataset(Dataset):\n",
        "    \"\"\"Whole Image dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_folder = \"images\"\n",
        "        self.mask_folder = \"mask\"\n",
        "        self.file_names = os.listdir(os.path.join(root_dir, self.image_folder))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.file_names[idx]\n",
        "\n",
        "        # Load image\n",
        "        img_path = os.path.join(self.root_dir, self.image_folder, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Load mask\n",
        "        mask_path = os.path.join(self.root_dir, self.mask_folder, img_name)\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = self.transform(mask)\n",
        "\n",
        "        return {\"image\": image, \"mask\": mask}\n"
      ],
      "metadata": {
        "id": "QMSTaVbBiPGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchDataset(Dataset):\n",
        "    \"\"\"Patch dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.file_names = os.listdir(self.root_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.file_names[idx]\n",
        "\n",
        "        # Load image\n",
        "        img_path = os.path.join(self.root_dir, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image\n"
      ],
      "metadata": {
        "id": "Pcbo1ABd443B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DRIVEWholeModule(pl.LightningDataModule):\n",
        "  def __init__(\n",
        "      self,\n",
        "      k: int = 1,  # fold number\n",
        "      split_seed: int = 42,  # split needs to be always the same for correct cross validation\n",
        "      num_splits: int = 5,\n",
        "      batch_size: int = 24,\n",
        "      num_workers: int = 0,\n",
        "      pin_memory: bool = True\n",
        "    ):\n",
        "    super().__init__()\n",
        "    # this line allows to access init params with 'self.hparams' attribute\n",
        "    self.save_hyperparameters(logger=False)\n",
        "    self.dir = '/content/DRIVE/'\n",
        "    self.train_dir = os.path.join(self.dir, \"training\")\n",
        "    self.test_dir = os.path.join(self.dir, \"test\")\n",
        "    self.transform = transforms.Compose([\n",
        "      transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "\n",
        "  def setup(self, stage: str):\n",
        "      # Assign train/val datasets for use in dataloaders\n",
        "      if stage == \"fit\":\n",
        "          drive_full = WholeImageDataset(self.train_dir, transform=self.transform)\n",
        "\n",
        "          # choose fold to train on\n",
        "          kf = KFold(n_splits=self.hparams.num_splits, shuffle=True, random_state=self.hparams.split_seed)\n",
        "          all_splits = [k for k in kf.split(drive_full)]\n",
        "          train_indexes, val_indexes = all_splits[self.hparams.k]\n",
        "          train_indexes, val_indexes = train_indexes.tolist(), val_indexes.tolist()\n",
        "\n",
        "          self.train_data, self.val_data = drive_full[train_indexes], drive_full[val_indexes]\n",
        "\n",
        "      # Assign test dataset for use in dataloader(s)\n",
        "      if stage == \"test\":\n",
        "          self.test_data = WholeImageDataset(self.test_dir, transform=self.transform)\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.train_data, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers,\n",
        "                          pin_memory=self.hparams.pin_memory, shuffle=True)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.val_data, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers,\n",
        "                          pin_memory=self.hparams.pin_memory)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(self.test_data, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers,\n",
        "                          pin_memory=self.hparams.pin_memory)"
      ],
      "metadata": {
        "id": "o35vH8mxs-06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DRIVEPatchModule(pl.LightningDataModule):\n",
        "  def __init__(\n",
        "      self,\n",
        "      k: int = 1,  # fold number\n",
        "      split_seed: int = 42,  # split needs to be always the same for correct cross validation\n",
        "      num_splits: int = 5,\n",
        "      batch_size: int = 24,\n",
        "      num_workers: int = 0,\n",
        "      pin_memory: bool = True\n",
        "    ):\n",
        "    super().__init__()\n",
        "    self.dir = '/content/DRIVE_patches/'\n",
        "    self.train_dir = os.path.join(self.dir, \"training\")\n",
        "    self.test_dir = os.path.join(self.dir, \"test\")\n",
        "    self.transform = transforms.Compose([\n",
        "      transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "\n",
        "  def setup(self, stage: str):\n",
        "      # Assign train/val datasets for use in dataloaders\n",
        "      if stage == \"fit\":\n",
        "          drive_full = PatchDataset(self.train_dir, transform=self.transform)\n",
        "\n",
        "          # choose fold to train on\n",
        "          kf = KFold(n_splits=self.hparams.num_splits, shuffle=True, random_state=self.hparams.split_seed)\n",
        "          all_splits = [k for k in kf.split(drive_full)]\n",
        "          train_indexes, val_indexes = all_splits[self.hparams.k]\n",
        "          train_indexes, val_indexes = train_indexes.tolist(), val_indexes.tolist()\n",
        "\n",
        "          self.train_data, self.val_data = drive_full[train_indexes], drive_full[val_indexes]\n",
        "\n",
        "      # Assign test dataset for use in dataloader(s)\n",
        "      if stage == \"test\":\n",
        "          drive_full = PatchDataset(self.test_dir, transform=self.transform)\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.train_data, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers,\n",
        "                          pin_memory=self.hparams.pin_memory, shuffle=True)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.val_data, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers,\n",
        "                          pin_memory=self.hparams.pin_memory)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(self.test_data, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers,\n",
        "                          pin_memory=self.hparams.pin_memory)"
      ],
      "metadata": {
        "id": "5AM4KdGg-9j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CHASEPatchModule(pl.LightningDataModule):\n",
        "  def __init__(\n",
        "      self,\n",
        "      k: int = 1,  # fold number\n",
        "      split_seed: int = 42,  # split needs to be always the same for correct cross validation\n",
        "      num_splits: int = 5,\n",
        "      batch_size: int = 24,\n",
        "      num_workers: int = 0,\n",
        "      pin_memory: bool = True\n",
        "    ):\n",
        "    super().__init__()\n",
        "    self.dir = '/content/CHASE_patches/'\n",
        "    self.train_dir = os.path.join(self.dir, \"training\")\n",
        "    self.test_dir = os.path.join(self.dir, \"test\")\n",
        "    self.transform = transforms.Compose([\n",
        "      transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "\n",
        "  def setup(self, stage: str):\n",
        "      # Assign train/val datasets for use in dataloaders\n",
        "      if stage == \"fit\":\n",
        "          chase_full = PatchDataset(self.train_dir, transform=self.transform)\n",
        "\n",
        "          # choose fold to train on\n",
        "          kf = KFold(n_splits=self.hparams.num_splits, shuffle=True, random_state=self.hparams.split_seed)\n",
        "          all_splits = [k for k in kf.split(chase_full)]\n",
        "          train_indexes, val_indexes = all_splits[self.hparams.k]\n",
        "          train_indexes, val_indexes = train_indexes.tolist(), val_indexes.tolist()\n",
        "\n",
        "          self.train_data, self.val_data = chase_full[train_indexes], chase_full[val_indexes]\n",
        "\n",
        "      # Assign test dataset for use in dataloader(s)\n",
        "      if stage == \"test\":\n",
        "          chase_full = PatchDataset(self.test_dir, transform=self.transform)\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.train_data, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers,\n",
        "                          pin_memory=self.hparams.pin_memory, shuffle=True)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.val_data, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers,\n",
        "                          pin_memory=self.hparams.pin_memory)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(self.test_data, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers,\n",
        "                          pin_memory=self.hparams.pin_memory)"
      ],
      "metadata": {
        "id": "cuHbdkhv_JE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CHASEWholeModule(pl.LightningDataModule):\n",
        "  def __init__(\n",
        "      self,\n",
        "      k: int = 1,  # fold number\n",
        "      split_seed: int = 42,  # split needs to be always the same for correct cross validation\n",
        "      num_splits: int = 5,\n",
        "      batch_size: int = 24,\n",
        "      num_workers: int = 0,\n",
        "      pin_memory: bool = True\n",
        "    ):\n",
        "    super().__init__()\n",
        "    self.dir = '/content/CHASE/'\n",
        "    self.train_dir = os.path.join(self.dir, \"training\")\n",
        "    self.test_dir = os.path.join(self.dir, \"test\")\n",
        "    self.transform = transforms.Compose([\n",
        "      transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "\n",
        "  def setup(self, stage: str):\n",
        "      # Assign train/val datasets for use in dataloaders\n",
        "      if stage == \"fit\":\n",
        "          chase_full = WholeImageDataset(self.train_dir, transform=self.transform)\n",
        "\n",
        "          # choose fold to train on\n",
        "          kf = KFold(n_splits=self.hparams.num_splits, shuffle=True, random_state=self.hparams.split_seed)\n",
        "          all_splits = [k for k in kf.split(chase_full)]\n",
        "          train_indexes, val_indexes = all_splits[self.hparams.k]\n",
        "          train_indexes, val_indexes = train_indexes.tolist(), val_indexes.tolist()\n",
        "\n",
        "          self.train_data, self.val_data = chase_full[train_indexes], chase_full[val_indexes]\n",
        "\n",
        "      # Assign test dataset for use in dataloader(s)\n",
        "      if stage == \"test\":\n",
        "          chase_full = WholeImageDataset(self.test_dir, transform=self.transform)\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.train_data, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers,\n",
        "                          pin_memory=self.hparams.pin_memory, shuffle=True)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.val_data, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers,\n",
        "                          pin_memory=self.hparams.pin_memory)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(self.test_data, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers,\n",
        "                          pin_memory=self.hparams.pin_memory)"
      ],
      "metadata": {
        "id": "cGSDrb7P-igT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class STAREWholeModule(pl.LightningDataModule):\n",
        "  def __init__(\n",
        "      self,\n",
        "      k: int = 1,  # fold number\n",
        "      split_seed: int = 42,  # split needs to be always the same for correct cross validation\n",
        "      num_splits: int = 5,\n",
        "      batch_size: int = 24,\n",
        "      num_workers: int = 0,\n",
        "      pin_memory: bool = True\n",
        "    ):\n",
        "    super().__init__()\n",
        "    self.dir = '/content/STARE/'\n",
        "    self.train_dir = os.path.join(self.dir, \"training\")\n",
        "    self.test_dir = os.path.join(self.dir, \"test\")\n",
        "    self.transform = transforms.Compose([\n",
        "      transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "\n",
        "  def setup(self, stage: str):\n",
        "      # Assign train/val datasets for use in dataloaders\n",
        "      if stage == \"fit\":\n",
        "          stare_full = WholeImageDataset(self.train_dir, transform=self.transform)\n",
        "\n",
        "          # choose fold to train on\n",
        "          kf = KFold(n_splits=self.hparams.num_splits, shuffle=True, random_state=self.hparams.split_seed)\n",
        "          all_splits = [k for k in kf.split(stare_full)]\n",
        "          train_indexes, val_indexes = all_splits[self.hparams.k]\n",
        "          train_indexes, val_indexes = train_indexes.tolist(), val_indexes.tolist()\n",
        "\n",
        "          self.train_data, self.val_data = stare_full[train_indexes], stare_full[val_indexes]\n",
        "\n",
        "      # Assign test dataset for use in dataloader(s)\n",
        "      if stage == \"test\":\n",
        "          stare_full = WholeImageDataset(self.test_dir, transform=self.transform)\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.train_data, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers,\n",
        "                          pin_memory=self.hparams.pin_memory, shuffle=True)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.val_data, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers,\n",
        "                          pin_memory=self.hparams.pin_memory)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(self.test_data, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers,\n",
        "                          pin_memory=self.hparams.pin_memory)"
      ],
      "metadata": {
        "id": "hklfBwfK-ipR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class STAREPatchModule(pl.LightningDataModule):\n",
        "  def __init__(\n",
        "      self,\n",
        "      k: int = 1,  # fold number\n",
        "      split_seed: int = 42,  # split needs to be always the same for correct cross validation\n",
        "      num_splits: int = 5,\n",
        "      batch_size: int = 24,\n",
        "      num_workers: int = 0,\n",
        "      pin_memory: bool = True\n",
        "    ):\n",
        "    super().__init__()\n",
        "    self.dir = '/content/STARE_patches/'\n",
        "    self.train_dir = os.path.join(self.dir, \"training\")\n",
        "    self.test_dir = os.path.join(self.dir, \"test\")\n",
        "    self.transform = transforms.Compose([\n",
        "      transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "\n",
        "  def setup(self, stage: str):\n",
        "      # Assign train/val datasets for use in dataloaders\n",
        "      if stage == \"fit\":\n",
        "          stare_full = PatchDataset(self.train_dir, transform=self.transform)\n",
        "\n",
        "          # choose fold to train on\n",
        "          kf = KFold(n_splits=self.hparams.num_splits, shuffle=True, random_state=self.hparams.split_seed)\n",
        "          all_splits = [k for k in kf.split(stare_full)]\n",
        "          train_indexes, val_indexes = all_splits[self.hparams.k]\n",
        "          train_indexes, val_indexes = train_indexes.tolist(), val_indexes.tolist()\n",
        "\n",
        "          self.train_data, self.val_data = stare_full[train_indexes], stare_full[val_indexes]\n",
        "\n",
        "      # Assign test dataset for use in dataloader(s)\n",
        "      if stage == \"test\":\n",
        "          stare_full = PatchDataset(self.test_dir, transform=self.transform)\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.train_data, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers,\n",
        "                          pin_memory=self.hparams.pin_memory, shuffle=True)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.val_data, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers,\n",
        "                          pin_memory=self.hparams.pin_memory)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(self.test_data, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers,\n",
        "                          pin_memory=self.hparams.pin_memory)"
      ],
      "metadata": {
        "id": "hoojujHw_Nv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Blocks"
      ],
      "metadata": {
        "id": "RN0JLlchIc4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downsample Block"
      ],
      "metadata": {
        "id": "VZ2E-coRNhF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downsample\n",
        "class DownsampleBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "      super(DownsampleBlock, self).__init__()\n",
        "\n",
        "      self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=(4, 4), stride=2, dilation=1)\n",
        "      self.bn = nn.BatchNorm2d(out_channels)\n",
        "      self.leaky = nn.LeakyReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "      out = self.conv(x)\n",
        "      out = self.bn(x)\n",
        "      out = self.leaky(x)\n",
        "      return out\n",
        "\n"
      ],
      "metadata": {
        "id": "Fly_BL92ITDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upsample Block"
      ],
      "metadata": {
        "id": "6unbFS7ZNjAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upsample\n",
        "class UpsampleBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "      super(UpsampleBlock, self).__init__()\n",
        "\n",
        "      self.conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=(4, 4), stride=2, dilation=1)\n",
        "      self.bn = nn.BatchNorm2d(out_channels)\n",
        "      self.leaky = nn.LeakyReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "      out = self.conv(x)\n",
        "      out = self.bn(out)\n",
        "      out = self.leaky(out)\n",
        "      return out"
      ],
      "metadata": {
        "id": "mm-HaW-LNlQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SFA Block"
      ],
      "metadata": {
        "id": "vuP31xNANmIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SFA Block\n",
        "\n",
        "# Had to implement this because the function exists in TF but not in PyTorch\n",
        "# See https://stackoverflow.com/questions/65154182/implement-separableconv2d-in-pytorch\n",
        "class SeparableConv2d(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride, dilation, bias=False):\n",
        "      super(SeparableConv2d, self).__init__()\n",
        "      self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size,\n",
        "                                groups = in_channels, bias = bias, stride = stride, dilation = dilation)\n",
        "      self.pointwise = nn.Conv2d(in_channels, out_channels,\n",
        "                                kernel_size = 1, bias = bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "      out = self.depthwise(x)\n",
        "      out = self.pointwise(out)\n",
        "      return out\n",
        "\n",
        "\n",
        "class SFABlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "      super(SFABlock, self).__init__()\n",
        "\n",
        "      self.sepconv = SeparableConv2d(in_channels, out_channels, kernel_size=(3, 3), stride=1, dilation=1)\n",
        "      self.bn = nn.BatchNorm2d(out_channels)\n",
        "      self.leaky = nn.LeakyReLU()\n",
        "      self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), stride=1, dilation=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "      first = self.sepconv(x)\n",
        "      first = self.bn(first)\n",
        "      first = self.leaky(first)\n",
        "      res_first = torch.add(x, first)\n",
        "\n",
        "      second = self.conv(res_first)\n",
        "      second = self.bn(second)\n",
        "      second = self.leaky(second)\n",
        "      res_second = torch.add(res_first, second)\n",
        "\n",
        "      out = torch.add(res_second, x)\n",
        "\n",
        "      return out"
      ],
      "metadata": {
        "id": "1z8otr32Jklt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generator Residual Block"
      ],
      "metadata": {
        "id": "E3Fq5UtONny4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator Residual\n",
        "class GenResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "      super(GenResidualBlock, self).__init__()\n",
        "\n",
        "      self.reflect = nn.ReflectionPad2d(padding = 2)\n",
        "      self.sepconv_d1 = SeparableConv2d(in_channels, out_channels, kernel_size=(3, 3), stride=1, dilation=1)\n",
        "      self.sepconv_d2 = SeparableConv2d(in_channels, out_channels, kernel_size=(3, 3), stride=1, dilation=2)\n",
        "      self.bn = nn.BatchNorm2d(out_channels)\n",
        "      self.leaky = nn.LeakyReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "      first = self.reflect(x)\n",
        "      first = self.sepconv_d1(first)\n",
        "      first = self.bn(first)\n",
        "      first = self.leaky(first)\n",
        "\n",
        "      split_first = self.reflect(first)\n",
        "      split_first = self.sepconv_d1(split_first)\n",
        "      split_first = self.bn(split_first)\n",
        "      split_first = self.leaky(split_first)\n",
        "\n",
        "      split_second = self.reflect(first)\n",
        "      split_second = self.sepconv_d2(split_second)\n",
        "      split_second = self.bn(split_second)\n",
        "      split_second = self.leaky(split_second)\n",
        "\n",
        "      out = torch.add(x, split_first, split_second)\n",
        "\n",
        "      return out"
      ],
      "metadata": {
        "id": "z2Ogm6UrNpan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discriminator Residual Block"
      ],
      "metadata": {
        "id": "Ae6lmB7pO14c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Discriminator Residual\n",
        "class DisResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "      super(DisResidualBlock, self).__init__()\n",
        "\n",
        "      self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=(2, 2), stride=1, dilation=2)\n",
        "      self.sepconv = SeparableConv2d(in_channels, out_channels, kernel_size=(2, 2), stride=1, dilation=2)\n",
        "      self.bn = nn.BatchNorm2d(out_channels)\n",
        "      self.leaky = nn.LeakyReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "      split_first = self.conv(x)\n",
        "      split_first = self.bn(split_first)\n",
        "      split_first = self.leaky(split_first)\n",
        "\n",
        "      split_second = self.sepconv(x)\n",
        "      split_second = self.bn(split_second)\n",
        "      split_second = self.leaky(split_second)\n",
        "\n",
        "      out = torch.add(split_first, split_second)\n",
        "\n",
        "      return out"
      ],
      "metadata": {
        "id": "hFvWGMg5O1Ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generator G<sub>f<sub>"
      ],
      "metadata": {
        "id": "8v5ZJjFsbMLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator Gf\n",
        "\n",
        "class GeneratorFine(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GeneratorFine, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=(2, 2), stride=1, dilation=2)\n",
        "        self.downsample = DownsampleBlock(in_channels, out_channels)\n",
        "        self.upsample = UpsampleBlock(in_channels, out_channels)\n",
        "        self.sfa = SFABlock(in_channels, out_channels)\n",
        "        self.g_res = GenResidualBlock(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, mid):\n",
        "        x = self.conv(x)\n",
        "        first_out = self.downsample(x)\n",
        "\n",
        "        top = self.sfa(first_out)\n",
        "\n",
        "        bot = torch.add(x, mid)\n",
        "        bot = self.g_res(bot)\n",
        "        bot = self.g_res(bot)\n",
        "        bot = self.g_res(bot)\n",
        "\n",
        "        out = torch.cat((top, bot), axis = 0)\n",
        "        out = self.upsample(out)\n",
        "        out = self.conv(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "IytED4V1rgni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generator G<sub>c<sub>"
      ],
      "metadata": {
        "id": "DME8bi2JdaYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator Gc\n",
        "\n",
        "class GeneratorCoarse(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GeneratorCoarse, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=(2, 2), stride=1, dilation=2)\n",
        "        self.downsample = DownsampleBlock(in_channels, out_channels)\n",
        "        self.upsample = UpsampleBlock(in_channels, out_channels)\n",
        "        self.sfa = SFABlock(in_channels, out_channels)\n",
        "        self.g_res = GenResidualBlock(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        first_out = self.downsample(x)\n",
        "\n",
        "        top = self.sfa(first_out)\n",
        "\n",
        "        mid = self.downsample(x)\n",
        "        mid = self.sfa(mid)\n",
        "\n",
        "        bot = self.g_res(mid)\n",
        "        bot = self.g_res(bot)\n",
        "        bot = self.g_res(bot)\n",
        "        bot = self.g_res(bot)\n",
        "        bot = self.g_res(bot)\n",
        "        bot = self.g_res(bot)\n",
        "        bot = self.g_res(bot)\n",
        "        bot = self.g_res(bot)\n",
        "        bot = self.g_res(bot)\n",
        "\n",
        "        out = torch.cat((mid, bot), axis = 0)\n",
        "        out = self.upsample(out)\n",
        "\n",
        "\n",
        "        out = torch.cat((top, out), axis = 0)\n",
        "        out = self.upsample(out)\n",
        "        return out # To return value for Gf"
      ],
      "metadata": {
        "id": "ruiZ3AB0daYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discriminator D<sub>f<sub>"
      ],
      "metadata": {
        "id": "K6QvH9ePeoVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Discriminator Df\n",
        "\n",
        "class DiscriminatorFine(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DiscriminatorFine, self).__init__()\n",
        "        self.downsample = DownsampleBlock(in_channels, out_channels)\n",
        "        self.upsample = UpsampleBlock(in_channels, out_channels)\n",
        "        self.d_res = GenResidualBlock(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        first_out = self.downsample(x)\n",
        "\n",
        "        top = self.sfa(first_out)\n",
        "\n",
        "        bot = torch.add(x, mid)\n",
        "        bot = self.g_res(bot)\n",
        "        bot = self.g_res(bot)\n",
        "        bot = self.g_res(bot)\n",
        "\n",
        "        out = torch.cat((top, bot), axis = 0)\n",
        "        out = self.upsample(out)\n",
        "        out = self.conv(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "uPDdR_EheoVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discriminator D<sub>c<sub>"
      ],
      "metadata": {
        "id": "RWPHbw1neoVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Discriminator Dc\n",
        "\n",
        "class DiscriminatorCoarse(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DiscriminatorCoarse, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=(2, 2), stride=1, dilation=2)\n",
        "        self.downsample = DownsampleBlock(in_channels, out_channels)\n",
        "        self.upsample = UpsampleBlock(in_channels, out_channels)\n",
        "        self.sfa = SFABlock(in_channels, out_channels)\n",
        "        self.g_res = GenResidualBlock(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        first_out = self.downsample(x)\n",
        "\n",
        "        top = self.sfa(first_out)\n",
        "\n",
        "        mid = self.downsample(x)\n",
        "        mid = self.sfa(mid)\n",
        "\n",
        "        bot = self.g_res(mid)\n",
        "        bot = self.g_res(bot)\n",
        "        bot = self.g_res(bot)\n",
        "        bot = self.g_res(bot)\n",
        "        bot = self.g_res(bot)\n",
        "        bot = self.g_res(bot)\n",
        "        bot = self.g_res(bot)\n",
        "        bot = self.g_res(bot)\n",
        "        bot = self.g_res(bot)\n",
        "\n",
        "        out = torch.cat((mid, bot), axis = 0)\n",
        "        out = self.upsample(out)\n",
        "\n",
        "\n",
        "        out = torch.cat((top, out), axis = 0)\n",
        "        out = self.upsample(out)\n",
        "        return out # To return value for Gf"
      ],
      "metadata": {
        "id": "uJU8i-M3eoVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GAN(L.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels,\n",
        "        width,\n",
        "        height,\n",
        "        latent_dim: int = 100,\n",
        "        lr: float = 0.0002,\n",
        "        b1: float = 0.5,\n",
        "        b2: float = 0.999,\n",
        "        batch_size: int = 24,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.automatic_optimization = False\n",
        "\n",
        "        # networks\n",
        "        data_shape = (channels, width, height)\n",
        "        self.generator = Generator(latent_dim=self.hparams.latent_dim, img_shape=data_shape)\n",
        "        self.discriminator = Discriminator(img_shape=data_shape)\n",
        "\n",
        "        self.validation_z = torch.randn(8, self.hparams.latent_dim)\n",
        "\n",
        "        self.example_input_array = torch.zeros(2, self.hparams.latent_dim)\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.generator(z)\n",
        "\n",
        "    def adversarial_loss(self, y_hat, y): # TODO: Change to paper's novel loss\n",
        "        return F.binary_cross_entropy(y_hat, y)\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        imgs, _ = batch\n",
        "\n",
        "        optimizer_g, optimizer_d = self.optimizers()\n",
        "\n",
        "        # sample noise\n",
        "        z = torch.randn(imgs.shape[0], self.hparams.latent_dim)\n",
        "        z = z.type_as(imgs)\n",
        "\n",
        "        # train generator\n",
        "        # generate images\n",
        "        self.toggle_optimizer(optimizer_g)\n",
        "        self.generated_imgs = self(z)\n",
        "\n",
        "        # log sampled images\n",
        "        sample_imgs = self.generated_imgs[:6]\n",
        "        grid = make_grid(sample_imgs)\n",
        "        self.logger.experiment.add_image(\"generated_images\", grid, 0)\n",
        "\n",
        "        # ground truth result (ie: all fake)\n",
        "        # put on GPU because we created this tensor inside training_loop\n",
        "        valid = torch.ones(imgs.size(0), 1)\n",
        "        valid = valid.type_as(imgs)\n",
        "\n",
        "        # adversarial loss is binary cross-entropy\n",
        "        g_loss = self.adversarial_loss(self.discriminator(self(z)), valid)\n",
        "        self.log(\"g_loss\", g_loss, prog_bar=True)\n",
        "        self.manual_backward(g_loss)\n",
        "        optimizer_g.step()\n",
        "        optimizer_g.zero_grad()\n",
        "        self.untoggle_optimizer(optimizer_g)\n",
        "\n",
        "        # train discriminator\n",
        "        # Measure discriminator's ability to classify real from generated samples\n",
        "        self.toggle_optimizer(optimizer_d)\n",
        "\n",
        "        # how well can it label as real?\n",
        "        valid = torch.ones(imgs.size(0), 1)\n",
        "        valid = valid.type_as(imgs)\n",
        "\n",
        "        real_loss = self.adversarial_loss(self.discriminator(imgs), valid)\n",
        "\n",
        "        # how well can it label as fake?\n",
        "        fake = torch.zeros(imgs.size(0), 1)\n",
        "        fake = fake.type_as(imgs)\n",
        "\n",
        "        fake_loss = self.adversarial_loss(self.discriminator(self(z).detach()), fake)\n",
        "\n",
        "        # discriminator loss is the average of these\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "        self.log(\"d_loss\", d_loss, prog_bar=True)\n",
        "        self.manual_backward(d_loss)\n",
        "        optimizer_d.step()\n",
        "        optimizer_d.zero_grad()\n",
        "        self.untoggle_optimizer(optimizer_d)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        lr = self.hparams.lr\n",
        "        b1 = self.hparams.b1\n",
        "        b2 = self.hparams.b2\n",
        "\n",
        "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n",
        "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
        "        return [opt_g, opt_d], []\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        z = self.validation_z.type_as(self.generator.model[0].weight)\n",
        "\n",
        "        # log sampled images\n",
        "        sample_imgs = self(z)\n",
        "        grid = make_grid(sample_imgs)\n",
        "        self.logger.experiment.add_image(\"generated_images\", grid, self.current_epoch)"
      ],
      "metadata": {
        "id": "QmMNN6_qZ4BX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data variables\n",
        "drive_whole = DRIVEWholeModule()\n",
        "drive_patch = DRIVEPatchModule()\n",
        "chase_whole = CHASEWholeModule()\n",
        "chase_patch = CHASEPatchModule()\n",
        "stare_whole = STAREWholeModule()\n",
        "stare_patch = STAREPatchModule()\n",
        "\n",
        "\n",
        "# train\n",
        "model = GAN()\n"
      ],
      "metadata": {
        "id": "L_x_XJeYs9E-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time to train the model!"
      ],
      "metadata": {
        "id": "_flZkOyqyZsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = pl.Trainer(\n",
        "    callbacks=[ModelSummary(max_depth=1)],\n",
        "    accelerator=\"auto\",\n",
        "    max_epochs=10\n",
        "    )\n",
        "\n",
        "trainer.fit(model, drive_data)"
      ],
      "metadata": {
        "id": "lpSqj5DvyWFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir lightning_logs/"
      ],
      "metadata": {
        "id": "LnKRIehCxKQj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}